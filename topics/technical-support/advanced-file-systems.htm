<!DOCTYPE html>
<html lang="en"><head>
<link href="../../_assets/favicons/favicon.ico" rel="icon">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-bc185b5c5bdbcb35c2eb49d8a876ef70.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.33">

  <meta name="dcterms.date" content="2024-11-25">
  <title>{{&lt; var course.title &gt;}} – Advanced file systems</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-0c8acc74dca0c2faaedea4940f202713.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Advanced file systems – Technical Support">
<meta property="og:description" content="RAID, ZFS, and the “next generation” of file systems">
<meta property="og:image" content="https://John-Abbott-College.github.io/1N6-Notes/topics/technical-support/_assets/favicons/favicon-32x32.png">
<meta property="og:site_name" content="{{< var course.title >}}">
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Advanced file systems</h1>
  <p class="subtitle">RAID, ZFS, and the “next generation” of file systems</p>

<div class="quarto-title-authors">
</div>

  <p class="date">2024-11-25</p>
</section>
<section class="slide level2">

<p><em>This lecture is directly adapted from the following article: <a href="https://arstechnica.com/information-technology/2014/01/bitrot-and-atomic-cows-inside-next-gen-filesystems/" class="uri">https://arstechnica.com/information-technology/2014/01/bitrot-and-atomic-cows-inside-next-gen-filesystems/</a></em></p>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>Most people don't care much about their filesystems. But at the end of the day, the filesystem is probably the single most important part of an operating system. A kernel bug might mean the loss of whatever you're working on right now, but a filesystem bug could wipe out everything you've ever done... and it could do so in ways most people never imagine.</p>
<p>Sound too theoretical to make you care about filesystems? Let's talk about "bitrot," the silent corruption of data on disk or tape. One at a time, year by year, a random bit here or there gets flipped. If you have a malfunctioning drive or controller—or a loose/faulty cable—a <em>lot</em> of bits might get flipped. Bitrot is a real thing, and it affects you more than you probably realize. The JPEG that ended in blocky weirdness halfway down? Bitrot. The MP3 that startled you with a violent CHIRP!, and you wondered if it had always done that? No, it probably hadn't—blame bitrot. The video with a bright green block in one corner followed by several seconds of weird rainbowy blocky stuff before it cleared up again? Bitrot.</p>
</section>
<section id="lesson-overview" class="slide level2">
<h2>Lesson Overview</h2>
</section>
<section id="raid" class="slide level2">
<h2>RAID</h2>
</section>
<section id="next-generation-file-systems" class="slide level2">
<h2>Next generation file systems</h2>
<p>Contrary to popular belief, conventional RAID won't help with bitrot, either. "But my raid5 array has parity and can reconstruct the missing data!" you might say. That only works if a drive <em>completely and cleanly fails.</em> If the drive instead starts spewing corrupted data, the array may or may not notice the corruption (most arrays don't check parity by default on every read). Even if it does notice... all the array knows is that something in the stripe is bad; it has no way of knowing <em>which</em> drive returned bad data—and therefore which one to rebuild from parity (or whether the parity block itself was corrupt).</p>
<h3 id="per-block-checksums">Per-block checksums</h3>
<h3 id="cow-copy-on-write">COW: Copy on Write</h3>
<h3 id="snapshots">Snapshots</h3>
<p>The worst thing is that backups won't save you from bitrot. The next backup will cheerfully back up the corrupted data, replacing your last <em>good</em> backup with the bad one. Before long, you'll have rotated through all of your backups (if you even have multiple backups), and the uncorrupted original is now gone for good.</p>
<p>What might save your data, however, is a "next-gen" filesystem.</p>
<p>Let's look at a graphic demonstration. Here's a picture of my son Finn that I like to call "Genesis of a Supervillain." I like this picture a lot, and I'd hate to lose it, which is why I store it on a next-gen filesystem with redundancy. But what if I didn't do that?</p>
<p>As a test, I set up a virtual machine with six drives. One has the operating system on it, two are configured as a simple btrfs-raid1 mirror, and the remaining three are set up as a conventional raid5. I saved Finn's picture on both the btrfs-raid1 mirror and the conventional raid5 array, and then I took the whole system offline and flipped a single bit—yes, just a single bit from 0 to 1—in the JPG file saved on each array. Here's the result:</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396069" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Original image.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396071" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-raid5-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Corrupted image: RAID5.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396067" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-btrfsraid1-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Corrupted image: btrfs-raid1.</p>
<p>The raid5 array didn't notice or didn't care about the flipped bit in Finn's picture any more than a standard single disk would. The next-gen btrfs-raid1 system, however, immediately caught <em>and corrected</em> the problem. The results are pretty obvious. If you care about your data, you want a next-gen filesystem. Here, we'll examine two: the older ZFS and the more recent btrfs.</p>
</section>
<section id="what-is-a-next-generation-filesystem-anyway" class="slide level2">
<h2>What is a “next-generation” filesystem, anyway?</h2>
<p>"Next-generation" is a phrase that gets handed out like sales flyers in a mall parking lot. But in this case, it actually means something. I define a "generation" of filesystems as a group that uses a particular "killer feature"—or closely related set of them—that earlier filesystems don't but that later filesystems all do. Let's take a quick trip down memory lane and examine past and current generations:</p>
<blockquote>
<p><strong>Generation 0:</strong>&nbsp;No system at all. There was just an arbitrary stream of data. Think punchcards, data on audiocassette, Atari 2600 ROM carts.</p>
<p><strong>Generation 1:</strong>&nbsp;Early random access. Here, there are multiple named files on one device with no folders or other metadata. Think Apple ][ DOS (but not ProDOS!) as one example.</p>
<p><strong>Generation 2:</strong>&nbsp;Early organization (aka folders). When devices became capable of holding hundreds of files, better organization became necessary. We're referring to TRS-DOS, Apple //c ProDOS, MS-DOS FAT/FAT32, etc.</p>
<p><strong>Generation 3:</strong>&nbsp;Metadata—ownership, permissions, etc. As the user count on machines grew higher, the ability to restrict and control access became necessary. This includes AT&amp;T UNIX, Netware, early NTFS, etc.</p>
<p><strong>Generation 4:</strong> Journaling! This is the killer feature defining all current, modern filesystems—ext4, modern NTFS, UFS2, you name it. Journaling keeps the filesystem from becoming inconsistent in the event of a crash, making it much less likely that you'll lose data, or even an entire disk, when the power goes off or the kernel crashes.</p>
</blockquote>
<p>So if you accept my definition, "next-generation" currently means "fifth generation." It's defined by an entire set of features: built-in volume management, per-block checksumming, self-healing redundant arrays, atomic COW snapshots, asynchronous replication, and far-future scalability.</p>
<p>That's quite a laundry list, and one or two individual features from it have shown up in some "current-gen" systems (Windows has Volume Shadow Copy to correspond with snapshots, for example). But there's a strong case to be made for the entire list defining the next generation.</p>
</section>
<section id="justify-your-generation" class="slide level2">
<h2>Justify your generation</h2>
<p>The quickest objection you could make to defining "generations" like this would be to point at NTFS' Volume Snapshot Service (VSS) or at the Linux Logical Volume Manager (LVM), each of which can take snapshots of filesystems mounted beneath them. However, these snapshots can't be replicated incrementally, meaning that backing up 1TB of data requires groveling over 1TB of data every time you do it. (FreeBSD's UFS2 also offered limited snapshot capability.) Worse yet, you generally can't replicate them as snapshots<em>—</em>with references intact—which means that your remote storage requirements increase exponentially, and the difficulty of managing backups does as well. With ZFS or btrfs replicated snapshots, you can have a single, immediately browsable, fully functional filesystem with 1,000+ versions of the filesystem available simultaneously. Using VSS with Windows Backup, you must use VHD files as a target. Among other limitations, VHD files are only supported up to 2TiB in size, making them useless for even a single backup of a large disk or array. They must also be mounted with special tools not available on all versions of Windows, which goes even further to limit them as tools for specialists only.</p>
<p>Finally, Microsoft's VSS typically depends on "writer" components that interface with applications (such as MS SQL Server) which can themselves hang up, making it difficult to successfully create a VSS snapshot in some cases. To be fair, when working properly, VSS writers offer something that simple snapshots don't—application-level consistency. But VSS writer bugs are a real problem, and I've encountered lots of Windows Servers which were quietly failing to create Shadow Copies. (VSS does not automatically create a writer-less Shadow Copy if the system times out; it just logs the failure and gives up.) I have yet to encounter a ZFS or btrfs filesystem or array that won't immediately create a valid snapshot.</p>
<p>At the end of the day, both LVM and VSS offer useful features that a lot of sysadmins do use, but they don't jump right out and demand your attention the way filenames, folders, metadata, or journaling did when they came onto the market. Still, this is only one feature out of the entire laundry list. You could make a case that snapshots made the fifth generation, and the <em>other</em> features in ZFS and btrfs make the sixth. But by the time you finish this article, you'll see there's no way to argue that btrfs and ZFS definitely constitute a <em>new</em> generation that is easily distinguishable from everything before them.</p>
<p><span id="page-2" class="record-pageview" data-page="2" data-url="https://arstechnica.com/information-technology/2014/01/bitrot-and-atomic-cows-inside-next-gen-filesystems/"></span></p>
</section>
<section id="zfseldest-of-the-next-generation" class="slide level2">
<h2>ZFS—eldest of the next generation</h2>
<p>One argument for labeling "snapshots, volume management, checksumming, self-healing, replication, and scalability" as the definitive feature set for the next generation of filesystems is that btrfs isn't the first filesystem to implement them as a complete set. ZFS (created by Sun Microsystems before the Oracle acquisition) was first to market.</p>
<p>Interestingly, there was a lot of pushback in the Linux world against ZFS, mostly concerning some of these very same features. In particular, the volume management and data-healing capabilities were loudly denounced as a "rampant layering violation." In the traditional layering model used by the Linux world, the RAID controller shouldn't know or care about the filesystem, and the filesystem shouldn't know or care about the RAID controller. But data healing depends on the filesystem knowing about redundant copies of data blocks. If the first copy of a block of data read fails its checksum, the filesystem needs to know if a different copy is available to be read, verified, and rewritten. This just isn't possible without merging those layers. A year later, btrfs supported raid0, raid1, and raid10; six years later, raid5 and raid6 support were added to mainline (still in progress).</p>
<p>We'll be comparing btrfs and ZFS directly moving forward; they each have their pros and cons, both in an absolute sense and when compared to each other.</p>
</section>
<section id="next-generation-features" class="slide level2">
<h2>Next-generation features</h2>
<p>Before we look too much at actual usage in an on-the-command-line sense, let's go over the actual features mentioned. It's important to understand how they impact you as a user or sysadmin directly. Once you do, you'll have a better idea of why you do (or, just barely conceivably, don't) need and want them on your own equipment. All of these features are available on ZFS and btrfs.</p>
<h3 id="atomic-cow-snapshots">Atomic COW snapshots</h3>
<p>An "atomic COW snapshot"—easily the most hilarious-sounding feature ever to grace a filesystem—is an image of the entire filesystem in exactly the condition it was in at a given instant in time, no matter what else was transpiring at the time. So if you take a snapshot of a filesystem at 8:13 and 32 seconds pm on December 19, 2013, that snapshot will contain every single byte of that filesystem at exactly 8:13 and 32 seconds pm on December 19, 2013—period, no ifs, ands, or buts. This helps keep high-activity structures like databases consistent. As long as the database uses journaling (and if it doesn't, upgrade!), its journal will be consistent in the snapshot. Any partially completed transactions can be cleanly rolled back instead of leaving the database in an inconsistent state.</p>
<p>COW stands for Copy On Write, so this snapshot that you took does not occupy any storage space in and of itself. It's just a bunch of extra pointers to the same blocks that already contain your data. As you delete or change your data in production, however, the snapshot will retain its copies. Over time, the space taken up by the snapshot increases from "none" to "the amount of data contained in this snapshot and no other snapshot." "Atomic," in this sense, refers to the atom as being an "indivisible" unit (quiet, quantum physicists!).</p>
<p>Still not sure what a snapshot is or why you'd want it? Well, imagine you're about to do something potentially dangerous to your system like apply an automatic update to a big cranky application you rely on a lot and don't trust, or manually delete a bunch of stuff in system directories to try to uninstall a program that you can't remove normally. (We're talking big things that could go wrong and may be difficult or impossible to undo.) Before doing them... take a snapshot. Then, if $BigScaryProcedure goes wrong, roll back to the snapshot you just took. <em>Poof</em>, everything is peachy again.</p>
<p>In practice, I take a snapshot every hour on the hour on my own machines and delete the old snapshots as necessary to recover disk space. That gives me the best possible chance to recover from something unexpected going horribly, horribly wrong.</p>
<h3 id="per-block-checksumming">Per-block checksumming</h3>
<p>It's a common misconception to think that RAID protects data from corruption since it introduces redundancy. The reality is exactly the opposite: traditional RAID <em>increases</em> the likelihood of data corruption since it introduces more physical devices with more things to go wrong. What RAID does protect you from is data loss due to the instantaneous failure of a drive. But if the drive isn't so obliging as to just politely die on you and instead starts reading and/or writing bad data, you're still going to <em>get</em> that bad data. The RAID controller has no way of knowing if the data is bad since parity is written on a per-stripe basis and not a per-block basis. In theory (in practice, parity isn't always strictly checked on every read), a RAID controller could tell you that the data in a stripe was corrupt, but it would have no way of knowing if the actual corrupt data was on any given drive. We demonstrated this in the introduction. Although I was using a raid5 array with parity, the picture of my son stored on it was very visibly corrupted when a single bit changed from a 0 to a 1. Ouch.</p>
<p>By contrast, with every individual block of data written, btrfs also writes a checksum. With every individual block of data read, btrfs reads the associated checksum and verifies the block against it. This allows the filesystem (and you) to know immediately if data has been corrupted.</p>
<h3 id="self-healing-redundant-arrays">Self-healing redundant arrays</h3>
<p>We also demonstrated this in the introduction. The copy of my son's picture on the btrfs-raid1 array got a bit flipped, just like the copy on the raid5 array, but the btrfs array detected and immediately repaired the corrupt data as soon as I attempted to read it.</p>
<p>The system's kernel log tells the story there:</p>
<pre><code>[   87.030967] BTRFS info (device vdf): csum failed ino 258 off 0 csum 3377436548 private 796777854
[   87.031188] BTRFS info (device vdf): csum failed ino 258 off 0 csum 3377436548 private 796777854
[   87.031678] btrfs read error corrected: ino 258 off 0 (dev /dev/vde sector 267344)</code></pre>
<p>You have to love that.</p>
<p>Healing is made possible by combining per-block checksumming with (redundant) volume and drive management. If you aren't storing your data redundantly, all you can do with a checksum is realize that your data is corrupt. But if you <em>are</em> storing data redundantly either through mirroring (btrfs-raid1 or btrfs-raid10) or striping-with-parity (btrfs-raid5 or btrfs-raid6), btrfs can immediately detect corruption and repair it. In this case, when a corrupt block is detected, it's read from parity or from an alternate copy, which is also verified. If the reconstruction/alternate copy does pass verification, it's quietly handed to you while btrfs rewrites it over the corrupt version in the background.</p>
<p>This is an awesome feature, and (in ZFS) it has personally saved me from data loss on many occasions. It's not uncommon at all to see five, or 10, or 50 checksum errors on a disk that's been in service for a few years... and in some cases I've seen ZFS raidZ arrays with 100,000+ checksum errors on one drive—and no data lost or corrupted. If you care about your data, and especially if you care about your data surviving for decades or longer, you absolutely need this feature.</p>
<h3 id="volume-management">Volume management</h3>
<p>For decades, we've partitioned drives in order to make more granular use of the space on them. If you want 100GB for your operating system and 900GB for data storage (and you don't want overuse of your data storage to cause your OS problems), you partition your drives. But if you change your mind later, deciding you want 200GB for your operating system, you're in for some pain. You <em>can</em>&nbsp;resize partitions, but it's a potentially dangerous operation, and it frequently&nbsp;requires you to shut your system down while it's going on.</p>
<p>Volume management, by contrast, means being able to dynamically define multiple filesystems with attributes including (but not limited to) quota size, read-only or read-write, and mountpoint at the drop of a hat. They can be resized, renamed, cloned, duplicated, and have permissions changed instantly and safely. Most of these features were previously available to Linux users under LVM, the Linux Logical Volume Management stack. Btrfs pretty much makes LVM obsolete.</p>
<h3 id="far-future-scalability">Far-future scalability</h3>
<p>One concept common to each of the last generations of filesystem is that they've tended to be obviously tied to the prevalent media at the time. When all you have are punch cards and audio tapes, not having an actual filesystem at all makes a lot of sense. When you're almost entirely working with floppy drives, scaling past a few MB of storage doesn't look like something worth bothering with. Now, however, we're working with rapidly increasing storage size per drive, and large storage arrays are becoming more and more common. The question "but what do I <em>do</em> with 6 petabytes of data?" starts to look a lot less silly.</p>
<p>ZFS and btrfs both made the decision to plan from the start for what currently looks like absolutely ridiculous scale designs, since the system will almost certainly outlive the hardware we currently implement it on.</p>
<p>The designers of ZFS famously claimed that flipping every bit in a maximum-sized zpool would "require enough energy to boil every ocean on the planet." Btrfs didn't go quite <em>that</em> far, but it didn't really need to. Its current maximum filesystem size is 16 EiB. To put that in perspective, if you gave every single human being in New York City a brand-new 2TiB hard drive and some way to physically plug them all up together, you'd be at right about 16EiB—and you might or might not be able to power the resultant monstrosity with a dedicated 90MW nuclear reactor. That's probably future-forward enough for now.</p>
<h3 id="asynchronous-incremental-replication">Asynchronous incremental replication</h3>
<p>Last but certainly not least, we come to replication. Boiled down to a nutshell, asynchronous replication means that you can take an atomic snapshot of an entire filesystem and easily move the entire thing, block-by-block, on to a remote filesystem. Own two computers? Do all your work on computer A, take a snapshot, send the snapshot to computer B. Computer B now has an exact replica of every single bit stored in your filesystem as of the time you took the snapshot. The problem, of course, is that we're talking every single bit of data... so it clearly takes a long time to copy.</p>
<p>Which is where "incremental" comes in. Once you've replicated your initial snapshot from computer A to computer B, you take another snapshot. Your filesystem size as a whole might be 4TiB, but if you've only changed, say, 500MiB of data... then that 500MiB is all you have to transfer from computer A to computer B. Now you have replication of the second snapshot taking place in seconds or minutes instead of hours or days. Even better, unlike traditional "synchronization" methods, you don't need to crawl over the filesystem on both sides first to figure out what's changed. Computer A knows exactly what has changed between snapshots 1 and 2, and it can immediately begin squirting that data—and only that data—to computer B.</p>
<figure class="ars-img-shortcode">
<img src="https://cms.arstechnica.com/wp-content/uploads/2014/01/btrfs_replicated_snapshots.png" class="fullwidth full" decoding="async">
<figcaption>
Left pane: your whole PC, as of the top of each hour. Right pane: your whole PC, as of the top of each hour, on a remote PC.
</figcaption>
</figure>
<p>In practice, this means it's actually no-kidding feasible to back your entire computer up on an hourly basis to a remote location—even with terabytes of data and a relatively cheap-and-slow Internet connection.</p>
<p><span id="page-3" class="record-pageview" data-page="3" data-url="https://arstechnica.com/information-technology/2014/01/bitrot-and-atomic-cows-inside-next-gen-filesystems/"></span></p>
</section>
<section id="features-unique-to-btrfs" class="slide level2">
<h2>Features unique to btrfs</h2>
<p>In the last section, I outlined what I consider the "standard set of features" for next-gen filesystems. But btrfs brings some new things to the table that ZFS doesn't.</p>
<h3 id="file-level-cloning">File-level cloning</h3>
<p>This, frankly, is huge. I work a lot with virtual machines, which means I have lots and lots of tremendous files ranging in size from 5GB to 400GB (sometimes bigger) lying around various systems. "File-level cloning" means that I can make a fully writable clone copy of one of these several-hundred-gigabyte files in an instant.</p>
<pre><code>    me@server:~$ cp --reflink=always 200GB_virtual_machine_drive.qcow2 clone_of_200GB_virtual_machine_drive.qcow2</code></pre>
<p>The above command will "copy" that 200GB of data in milliseconds, and it's a deduplicated copy at that. At the time of copying, no actual additional drive space is needed; clone_of_200GB_virtual_machine_drive.qcow2 actually just points to all the same data blocks. As I write to clone_of_200GB_virtual_machine.qcow2, though (or to the original 200GB_virtual_machine_drive.qcow2), each block that I write to is written as a new block. Over time, the file gradually diverges from its parent as needed—but <em>only</em> as needed.</p>
<p>This feature also enables you to selectively copy big chunks of data out of snapshots back into your main filesystem without having to preemptively roll the whole thing back... and again, without taking up any extra disk space unless and until you actually modify the copied (technically, cloned) files. This really can be a lifesaver.</p>
<p>By contrast, if you want to get individual files out of a ZFS snapshot (or one of the more primitive snapshots in NTFS or LVM), you have to actually copy them block by block, both consuming extra space and potentially taking a tremendous amount of time if it's a whole lot of data. Wasted disk space aside, the 200GB virtual machine image I used in my example could easily take three hours or more to copy out of a ZFS snapshot. In practice, this means that with ZFS, you want to create tons and tons of child filesystems since you can roll an entire filesystem back to a snapshot of itself instantly, but picking and choosing individual files means painful copy operations. With btrfs, you don't need to do that. Snapshot the entire btrfs filesystem, then just copy stuff back out of it piecemeal if you like. I consider this to be a pretty killer feature.</p>
<h3 id="online-balancing">Online balancing</h3>
<p>This is one of the features that home users and hobbyists constantly twit ZFS for lacking. With ZFS, once you set up a RAIDZ array, it's immutable. You can repair it by replacing drives, but you can never expand it, or contract it, or change its level. (You can add another array to a pool, but that's rarely what hobbyists or small businesses actually want.)</p>
<p>Btrfs, on the other hand, allows you to do pretty much anything in terms of live storage reconfiguration. Let's say you want to set up a system with two drives in a btrfs-raid1 (mirror) array. First, you run through your Linux installation... and just do a perfectly standard installation to the first disk. Get it all up and running on the one disk. Don't worry about the other one. Got it all running? Time to make sure we know what partition number we're using:</p>
<pre><code>    me@machine:~$ sudo btrfs filesystem show
    Label: none uuid: c9c5e506-6b87-4741-9017-f416d2f2ae8c
        Total devices 1 FS bytes used 1.41GB
        devid   1 size 9.31GB used 3.54GB path /dev/vda1</code></pre>
<p>OK, we're using the first partition of the first drive, which in my case is /dev/vda1. Before we do anything else, we want to copy the partition table layout to our second drive and install the GRUB boot loader onto it so we can boot from it if we lose the first drive:</p>
<pre><code>    me@machine:~$ sudo sfdisk -d /dev/vda | sudo sfdisk /dev/vdb
    me@machine:~$ sudo grub-install /dev/vdb</code></pre>
<p>Now we add the second drive's first partition to our btrfs filesystem:</p>
<pre><code>    me@machine:~$ sudo btrfs device add /dev/vdb1 /</code></pre>
<p>And we rebalance the filesystem as a btrfs-raid1 mirror:</p>
<pre><code>    me@machine:~$ sudo btrfs balance start -dconvert=raid1 -mconvert=raid1 /</code></pre>
<p>The system starts chugging along, busily copying all of the data from our first drive to the second one. The command doesn't return until the operation is done, but—here's the beautiful part—the system remains usable while this is going on! Browse the Internet, play solitaire, open up more terminals... whatever you like. In a few moments, if this is a clean system, the operation is done. You now have a working btrfs-raid1 system.</p>
<p>This is impressive enough on its own, but you could have added a third disk and rebalanced -dconvert=raid5 -mconvert=raid5, a fourth disk and raid10 or raid6... whatever you like. You still <em>can.</em> Btrfs is perfectly happy converting any raid level to any other raid level on the fly while the system is running. This is a killer feature for hobbyists and admins at smaller organizations who can't afford to just build entire new systems and migrate data over.</p>
<h3 id="nodatacow">NODATACOW</h3>
<p>One of the more common complaints about next-generation filesystems is that if you hammer them extremely hard with a never-ending stream of really punishing random I/O, they will eventually fall down harder than simpler conventional filesystems. So, the thinking goes, btrfs and ZFS may not be the best choice for that über database server or 128-core/256GB RAM experimental botnet-in-a-box VM host you're thinking of building.</p>
<p>Btrfs addresses this, at least in part, by letting you set any given file or directory NODATACOW, which means that the file or files within a directory will not be handled copy-on-write like normal files under btrfs (or zfs) are. This alleviates some of the ultimate-performance concerns mentioned above.</p>
<p>That said, in my experience, the vast majority of workloads—including databases and VM hosts under most load conditions—perform just <em>fine</em> under COW filesystems, so I wouldn't rush to set this on everything. In what little testing I've done, turning NODATACOW on for an image directory containing a Windows Server 2008 R2 VM resulted in a paltry five to 10 percent performance increase&nbsp;<em>at best</em>&nbsp;in an HDTune Pro benchmark run against its storage. And you will be giving up per-block checksumming at least by doing so. It's there, and it's very cool that it's there, but like most things, don't start tuning until you're sure you <em>need</em> to start tuning.</p>
<h3 id="file-leveldirectory-level-compression">File-level/Directory-level compression</h3>
<p>ZFS offers compression, but it needs to be enabled on an entire filesystem level. Btrfs also offers compression with multiple algorithms (currently gz is the default, and lzo is available out of the box), but btrfs allows you to control compression at the filesystem, subvolume, directory, and even individual file level. You can also do neat things like choose <em>not</em> to compress files whose names end in .jpg or .jpeg or .avi since those are compressed formats already, and general-purpose compression can only make them larger and cost you unnecessary CPU cycles. (This is the <em>default</em> behavior—you don't even need to set it up!)</p>
<p>This is pretty awesome. And devs on the mailing list are talking about making the feature even smarter by scanning files for MIME headers and/or test-compressing the first few blocks on write, then writing the rest decompressed if the compression algorithm doesn't seem to be working well.</p>
</section>
<section id="using-the-features-a-quick-rundown" class="slide level2">
<h2>Using the features: A quick rundown</h2>
<p>Let's take a look at a real, if simple, btrfs filesystem:</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs filesystem show
    failed to open /dev/sr0: No medium found
    Label: none  uuid: e5398102-7f0f-41e6-92f8-bc05176aa3ae
        Total devices 1 FS bytes used 3.03GB
        devid    1 size 24.00GB used 6.04GB path /dev/vda1

    Btrfs v0.20-rc1</code></pre>
<p>This is the btrfs filesystem itself as seen on a brand-new, just-installed Ubuntu Saucy virtual machine. By itself, this doesn't seem all that informative, but it becomes a lot more useful if you're running multiple filesystems or a single filesystem on multiple physical devices, such as a btrfs-raid0/1/5/6/10 array.</p>
<p>Now let's take a look at the subvolumes within the filesystem:</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs sub list /
    ID 256 gen 199294 top level 5 path @
    ID 257 gen 199294 top level 5 path @home</code></pre>
<p>What we're looking at here is Ubuntu's default installation layout on btrfs: one subvolume for root and another for /home. This allows you (in fact, requires you) to snapshot the two volumes independently, which in turn makes it easier to only back up one or the other or only roll back one or the other. You could, for example, roll back your root filesystem after a disastrous upgrade without affecting data you had saved in your home directory. We can see this reflected in /etc/fstab as well:</p>
<pre><code>    # /etc/fstab: static file system information.
    #
    # Use 'blkid' to print the universally unique identifier for a
    # device; this may be used with UUID= as a more robust way to name devices
    # that works even if disks are added and removed. See fstab(5).
    #
    #                
    proc            /proc           proc    nodev,noexec,nosuid 0       0
    # / was on /dev/sda5 during installation
    UUID=bf9ea9b9-54a7-4efc-8003-6ac0b344c6b5 /               btrfs   defaults,subvol=@     0       1
    # /home was on /dev/sda5 during installation
    UUID=bf9ea9b9-54a7-4efc-8003-6ac0b344c6b5 /home           btrfs   defaults,subvol=@home 0       2</code></pre>
<p>Let's say you've got another computer with a btrfs filesystem and you want to back up your home directory from this computer to the other one. We'll use btrfs' built-in asynchronous replication for this. First, let's create a new subvolume underneath /home to contain the snapshots:</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs sub create /home/.snapshots
    Create subvolume '/home/.snapshots'</code></pre>
<p>Now, let's take our first snapshot:</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs sub snapshot -r /home /home/.snapshots/myfirstsnapshot
    Create a readonly snapshot of '/home' in '/home/.snapshots/myfirstsnapshot'</code></pre>
<p>We can see the new structure with another btrfs subvolume list:</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs sub list /
    ID 256 gen 199294 top level 5 path @
    ID 257 gen 199294 top level 5 path @home
    ID 849 gen 199310 top level 5 path @home/.snapshots
    ID 850 gen 199311 top level 5 path @home/.snapshots/myfirstsnapshot</code></pre>
<p>Now, let's replicate the snapshot to our second machine, which has convenient subvolumes named /backup and /backup/home and /backup/home/.snapshots.</p>
<pre><code>    me@virtual-machine:~$ sudo btrfs send /home/.snapshots/myfirstsnapshot | ssh second-machine sudo btrfs receive /backup/home/.snapshots</code></pre>
<p>In its simplest possible form, this demonstrates sending the snapshot—which contains the entire /home subvolume at the time it was taken, remember—and tunneling it through ssh to the btrfs filesystem on the other end. There's nothing too special about this yet. Sure, you're replicating the entire subvolume all at once, which is kind of neat, but you could accomplish this in all kinds of ways. Where this gets cool is when we take and send a <em>second</em> snapshot.</p>
<pre><code>    me@virtual-machine: sudo btrfs sub snapshot -r /home /home/.snapshots/mysecondsnapshot
    me@virtual-machine: sudo btrfs send -p /home/.snapshots/myfirstsnapshot /home/.snapshots/mysecondsnapshot | ssh second-machine btrfs receive /backup/home/.snapshots</code></pre>
<p>What we did here was send only the data that has <em>changed on-disk</em> between "myfirstsnapshot" and "mysecondsnapshot." What's especially awesome about this is that the system doesn't have to laboriously scan the disks to find things that have changed. It already knows what has or hasn't changed, so it can just immediately start spitting data out to the receive process on the other end. If you have a few text files to replicate, this might not matter. But if you have a terabyte or five of database binaries or virtual machine storage, this is a huge deal. Backups that would have taken hours or days and generated a tremendous amount of system load with older technologies can now complete in minutes with little or no extra load.</p>
<p>I have systems that routinely replicate a terabyte or more of data across cheap 3Mbps and slower Internet connections using this technology... sometimes, replicating several times per day. There is literally no other technology that can accomplish this. If you have a six-figure SAN that replicates off-site several times a day, incremental snapshot replication is how it does it.</p>
</section>
<section id="great-where-do-i-start-well..." class="slide level2">
<h2>Great! Where do I start? Well...</h2>
<pre><code>    WARNING! - Btrfs Btrfs v0.20-rc1 IS EXPERIMENTAL
    WARNING! - see http://btrfs.wiki.kernel.org before using</code></pre>
<p>You will see this message every time you create or mount a btrfs filesystem, and it really does mean what it says. All of the features I've outlined in this article are there, and they work, and they're amazing... but they currently come with a heaping helping of surprises, not all pleasant. While I have not lost any data to btrfs—including in some fairly extensive "drop-testing" explicitly designed to <em>try</em> to make it lose data—I have seen weird performance death spirals that needed reboots to bring the machine back. There are also deliberately-still-rough areas of the interface like redundant arrays refusing to boot without being fed special arguments if one disk is lost, even though the array is otherwise fine. These headaches will need to be smoothed out before btrfs is ready for prime time.</p>
<p>Further, while the on-disk format is now considered "stable" and is not expected to change, btrfs is still evolving rapidly enough that you should probably be using <em>really</em> bleeding-edge kernels if you want to test it. I began my journey with btrfs using the very latest kernel in Ubuntu's pipeline, and I ended up jumping two steps beyond that to one in the dailies... which is way outside my normal comfort zone. If you're a very experienced or just-plain-determined admin and you're willing to <em>make and use regular backups</em>, btrfs is ready for you to jump in, test, and even use directly. But if you're looking for a mature filesystem that's ready for normal users and admins to use day to day... it will get there, but it's not there yet.</p>
<p>If you're still determined to jump in and test and/or use btrfs—well, dive on in! Be sure to read the wiki at http://btrfs.wiki.kernel.org <em>thoroughly</em>, and I'd advise subscribing to the btrfs mailing list (referenced at the wiki) as well. The Bugzilla installation at kernel.org is also a must-have resource, both for looking through open bugs and submitting your own.</p>
</section>
<section id="the-final-takeaway" class="slide level2">
<h2>The final takeaway</h2>
<p>The most important part of computing in the long term is keeping your data accessible and intact. Far-future scalability minimizes the need for spurious on-disk format changes (which can trap your data behind legacy equipment) in the future. Easily accessible snapshots mean the danger of "oops I accidentally lost the whole file" errors are largely avoidable. Incremental replication means that even huge volumes of data can be gotten safely, easily, and cheaply to physically remote locations. And easy-to-setup, easy-to-maintain, redundant self-healing arrays finally put the ugly specter of bitrot in a much more manageable position as well.</p>
<figure class="ars-img-shortcode">
<img src="https://cms.arstechnica.com/wp-content/uploads/2014/01/btrfs_replicated_snapshots.png" class="fullwidth full" decoding="async">
<figcaption>
Each folder is the whole filesystem, fully browsable. Each folder is on a local server, and on a remote server, exactly the same. Are your backups this easy to understand and use?
</figcaption>
</figure>
<p>Btrfs and ZFS make keeping your data safe—against all the things that tend to kill it—more possible than it ever has been. It's easy to miss the importance at first glance, because most of us didn't have any digital data 20 years ago. In 1994, for most people, computers were computers and real life was real life, so we aren't missing the lack of it now. But in 2014, we document our real lives directly on our computers—increasingly, only on our computers. In 2034, the difference between current and last-generation filesystems will be at least as obvious in retrospect as the difference between Polaroids and real film is now.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396069" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-original-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Original image.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>Original image.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396077" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>One bit flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>One bit flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LWxlZnQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmlld2JveD0iMCAwIDQwIDQwIj48ZGVmcz48Y2xpcHBhdGggaWQ9ImFycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EiPjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoNDB2NDBIMHoiIC8+PC9jbGlwcGF0aD48L2RlZnM+PGcgZmlsbD0iY3VycmVudENvbG9yIiBjbGlwLXBhdGg9InVybCgjYXJyb3ctYmxvY2tzLXJpZ2h0X3N2Z19fYSkiPjxwYXRoIGQ9Ik0zMiAxNmg4djhoLTh6bS04IDhoOHY4aC04em0tOCA4aDh2OGgtOHptOC0yNGg4djhoLTh6bS04LThoOHY4aC04ek0wIDE2aDE2djhIMHoiIC8+PC9nPjwvc3ZnPg==" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-left"></p>
<p>Original image.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>One bit flipped.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396075" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-two-bits-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Two bits flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>Two bits flipped.</p>
<p><a href="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits.jpg" class="cursor-zoom-in" data-pswp-width="360" data-pswp-height="480" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits-300x400.jpg 300w" data-cropped="true" target="_blank"><img data-src="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits.jpg" class="ars-gallery-image" decoding="async" loading="lazy" aria-labelledby="caption-396073" srcset="https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits.jpg 360w, https://cdn.arstechnica.net/wp-content/uploads/2014/01/bitrot-rotted-three-bits-300x400.jpg 300w" sizes="auto, (max-width: 360px) 100vw, 360px" width="360" height="480"></a></p>
<p>Three bits flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>Three bits flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LWxlZnQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmlld2JveD0iMCAwIDQwIDQwIj48ZGVmcz48Y2xpcHBhdGggaWQ9ImFycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EiPjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoNDB2NDBIMHoiIC8+PC9jbGlwcGF0aD48L2RlZnM+PGcgZmlsbD0iY3VycmVudENvbG9yIiBjbGlwLXBhdGg9InVybCgjYXJyb3ctYmxvY2tzLXJpZ2h0X3N2Z19fYSkiPjxwYXRoIGQ9Ik0zMiAxNmg4djhoLTh6bS04IDhoOHY4aC04em0tOCA4aDh2OGgtOHptOC0yNGg4djhoLTh6bS04LThoOHY4aC04ek0wIDE2aDE2djhIMHoiIC8+PC9nPjwvc3ZnPg==" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-left"></p>
<p>Two bits flipped.</p>
<p><img data-src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iYXJzLWdhbGxlcnktY2FwdGlvbi1hcnJvdyBhcnMtZ2FsbGVyeS1jYXB0aW9uLWFycm93LXJpZ2h0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCA0MCA0MCI+PGRlZnM+PGNsaXBwYXRoIGlkPSJhcnJvdy1ibG9ja3MtcmlnaHRfc3ZnX19hIj48cGF0aCBmaWxsPSJub25lIiBkPSJNMCAwaDQwdjQwSDB6IiAvPjwvY2xpcHBhdGg+PC9kZWZzPjxnIGZpbGw9ImN1cnJlbnRDb2xvciIgY2xpcC1wYXRoPSJ1cmwoI2Fycm93LWJsb2Nrcy1yaWdodF9zdmdfX2EpIj48cGF0aCBkPSJNMzIgMTZoOHY4aC04em0tOCA4aDh2OGgtOHptLTggOGg4djhoLTh6bTgtMjRoOHY4aC04em0tOC04aDh2OGgtOHpNMCAxNmgxNnY4SDB6IiAvPjwvZz48L3N2Zz4=" class="ars-gallery-caption-arrow ars-gallery-caption-arrow-right"></p>
<p>Three bits flipped.</p>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.2,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/John-Abbott-College\.github\.io\/1N6-Notes\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>